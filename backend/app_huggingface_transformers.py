from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
from dotenv import load_dotenv
from gtts import gTTS
import io
import base64
import json
from datetime import datetime
import threading
import time

# Environment variables yÃ¼kle
load_dotenv()

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# Model yapÄ±landÄ±rmasÄ±
MODEL_CONFIG = {
    "model_name": "microsoft/DialoGPT-medium",  # VarsayÄ±lan model
    "max_length": 500,
    "temperature": 0.7,
    "do_sample": True,
    "pad_token_id": None
}

# Global model deÄŸiÅŸkenleri
tokenizer = None
model = None
generator = None

# Tarihi figÃ¼rler ve kiÅŸilikleri
HISTORICAL_FIGURES = {
    "fatih_sultan_mehmet": {
        "name": "Fatih Sultan Mehmet",
        "personality": "Ben Fatih Sultan Mehmet. 1453'te Ä°stanbul'u fetheden OsmanlÄ± padiÅŸahÄ±yÄ±m. Bilim, sanat ve strateji konularÄ±nda konuÅŸmayÄ± severim. Sert ama adil bir liderim.",
        "era": "15. yÃ¼zyÄ±l",
        "location": "Ä°stanbul, OsmanlÄ± Ä°mparatorluÄŸu",
        "system_prompt": "Sen Fatih Sultan Mehmet'sin. 1453'te Ä°stanbul'u fetheden OsmanlÄ± padiÅŸahÄ±sÄ±n. Bilim, sanat ve strateji konularÄ±nda uzmansÄ±n. Sert ama adil bir lider olarak konuÅŸ. Tarihi gerÃ§eklere dayalÄ± yanÄ±tlar ver. TÃ¼rkÃ§e konuÅŸ."
    },
    "ataturk": {
        "name": "Mustafa Kemal AtatÃ¼rk",
        "personality": "Ben Mustafa Kemal AtatÃ¼rk. TÃ¼rkiye Cumhuriyeti'nin kurucusu ve ilk cumhurbaÅŸkanÄ±yÄ±m. ModernleÅŸme, eÄŸitim ve baÄŸÄ±msÄ±zlÄ±k konularÄ±nda tutkulu bir liderim.",
        "era": "19-20. yÃ¼zyÄ±l",
        "location": "Ankara, TÃ¼rkiye",
        "system_prompt": "Sen Mustafa Kemal AtatÃ¼rk'sÃ¼n. TÃ¼rkiye Cumhuriyeti'nin kurucusu ve ilk cumhurbaÅŸkanÄ±sÄ±n. ModernleÅŸme, eÄŸitim ve baÄŸÄ±msÄ±zlÄ±k konularÄ±nda tutkulusun. Tarihi gerÃ§eklere dayalÄ± yanÄ±tlar ver. TÃ¼rkÃ§e konuÅŸ."
    },
    "napoleon": {
        "name": "Napolyon Bonaparte",
        "personality": "Ben Napolyon Bonaparte. FransÄ±z Ä°mparatoru ve bÃ¼yÃ¼k bir askeri dehayÄ±m. Strateji, savaÅŸ ve yÃ¶netim konularÄ±nda uzmanÄ±m.",
        "era": "18-19. yÃ¼zyÄ±l",
        "location": "Paris, Fransa",
        "system_prompt": "Sen Napolyon Bonaparte'sÄ±n. FransÄ±z Ä°mparatoru ve bÃ¼yÃ¼k bir askeri dehasÄ±n. Strateji, savaÅŸ ve yÃ¶netim konularÄ±nda uzmansÄ±n. Tarihi gerÃ§eklere dayalÄ± yanÄ±tlar ver. TÃ¼rkÃ§e konuÅŸ."
    }
}

def load_model():
    """Modeli yÃ¼kle"""
    global tokenizer, model, generator
    
    try:
        print(f"ğŸ”„ Model yÃ¼kleniyor: {MODEL_CONFIG['model_name']}")
        
        # Tokenizer ve model yÃ¼kle
        tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['model_name'])
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_CONFIG['model_name'],
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        # Pad token ayarla
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Generator pipeline oluÅŸtur
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        
        print("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
        
    except Exception as e:
        print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
        # Fallback model
        MODEL_CONFIG["model_name"] = "gpt2"
        load_model()

def generate_response(figure, user_message):
    """AI yanÄ±tÄ± oluÅŸtur"""
    try:
        # Sistem prompt + kullanÄ±cÄ± mesajÄ±
        prompt = f"{figure['system_prompt']}\n\nKullanÄ±cÄ±: {user_message}\n{figure['name']}:"
        
        # YanÄ±t oluÅŸtur
        response = generator(
            prompt,
            max_length=MODEL_CONFIG['max_length'],
            temperature=MODEL_CONFIG['temperature'],
            do_sample=MODEL_CONFIG['do_sample'],
            pad_token_id=tokenizer.pad_token_id,
            num_return_sequences=1
        )
        
        # YanÄ±tÄ± temizle
        generated_text = response[0]['generated_text']
        ai_response = generated_text.split(f"{figure['name']}:")[-1].strip()
        
        # EÄŸer yanÄ±t Ã§ok kÄ±sa veya boÅŸsa, fallback yanÄ±t
        if len(ai_response) < 10:
            ai_response = f"Merhaba! Ben {figure['name']}. {figure['personality']} Sorunuzu daha detaylÄ± sorabilir misiniz?"
        
        return ai_response
        
    except Exception as e:
        print(f"âŒ YanÄ±t oluÅŸturma hatasÄ±: {e}")
        return f"Merhaba! Ben {figure['name']}. Åu anda teknik bir sorun yaÅŸÄ±yorum, lÃ¼tfen daha sonra tekrar deneyin."

@app.route('/')
def home():
    return jsonify({
        "message": "Tarih-i Sima API'ye hoÅŸ geldiniz! (Hugging Face Transformers)",
        "version": "3.0.0",
        "model": MODEL_CONFIG["model_name"],
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "available_figures": list(HISTORICAL_FIGURES.keys())
    })

@app.route('/api/figures', methods=['GET'])
def get_figures():
    """Mevcut tarihi figÃ¼rleri listele"""
    return jsonify(HISTORICAL_FIGURES)

@app.route('/api/chat', methods=['POST'])
def chat():
    """Tarihi figÃ¼rle sohbet et"""
    try:
        data = request.get_json()
        figure_id = data.get('figure_id')
        message = data.get('message')
        
        if not figure_id or not message:
            return jsonify({"error": "figure_id ve message gerekli"}), 400
        
        if figure_id not in HISTORICAL_FIGURES:
            return jsonify({"error": "GeÃ§ersiz figÃ¼r ID'si"}), 400
        
        figure = HISTORICAL_FIGURES[figure_id]
        
        # Model yÃ¼klÃ¼ deÄŸilse yÃ¼kle
        if generator is None:
            load_model()
        
        # AI yanÄ±tÄ± oluÅŸtur
        ai_response = generate_response(figure, message)
        
        # TTS ile ses dosyasÄ± oluÅŸtur
        try:
            tts = gTTS(text=ai_response, lang='tr', slow=False)
            audio_buffer = io.BytesIO()
            tts.write_to_fp(audio_buffer)
            audio_buffer.seek(0)
            audio_base64 = base64.b64encode(audio_buffer.read()).decode()
        except Exception as e:
            print(f"TTS hatasÄ±: {e}")
            audio_base64 = None
        
        return jsonify({
            "response": ai_response,
            "figure_name": figure['name'],
            "audio": audio_base64,
            "timestamp": datetime.now().isoformat(),
            "model": MODEL_CONFIG["model_name"]
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/tts', methods=['POST'])
def text_to_speech():
    """Metni sese Ã§evir"""
    try:
        data = request.get_json()
        text = data.get('text')
        
        if not text:
            return jsonify({"error": "text gerekli"}), 400
        
        tts = gTTS(text=text, lang='tr', slow=False)
        audio_buffer = io.BytesIO()
        tts.write_to_fp(audio_buffer)
        audio_buffer.seek(0)
        audio_base64 = base64.b64encode(audio_buffer.read()).decode()
        
        return jsonify({
            "audio": audio_base64,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/models/switch', methods=['POST'])
def switch_model():
    """Model deÄŸiÅŸtir"""
    try:
        data = request.get_json()
        new_model = data.get('model_name')
        
        if not new_model:
            return jsonify({"error": "model_name gerekli"}), 400
        
        MODEL_CONFIG["model_name"] = new_model
        
        # Yeni modeli yÃ¼kle
        load_model()
        
        return jsonify({
            "message": f"Model {new_model} olarak deÄŸiÅŸtirildi",
            "current_model": MODEL_CONFIG["model_name"]
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/models/available', methods=['GET'])
def get_available_models():
    """Mevcut modelleri listele"""
    available_models = [
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-small",
        "microsoft/DialoGPT-large",
        "facebook/blenderbot-400M-distill",
        "facebook/blenderbot-1B-distill",
        "EleutherAI/gpt-neo-2.7B",
        "EleutherAI/gpt-neo-1.3B",
        "gpt2",
        "gpt2-medium",
        "gpt2-large"
    ]
    
    return jsonify({
        "models": available_models,
        "current_model": MODEL_CONFIG["model_name"],
        "recommended": "microsoft/DialoGPT-medium"
    })

@socketio.on('connect')
def handle_connect():
    print('KullanÄ±cÄ± baÄŸlandÄ±')
    emit('connected', {
        'message': 'Sunucuya baÅŸarÄ±yla baÄŸlandÄ±nÄ±z',
        'model': MODEL_CONFIG["model_name"]
    })

@socketio.on('disconnect')
def handle_disconnect():
    print('KullanÄ±cÄ± ayrÄ±ldÄ±')

@socketio.on('chat_message')
def handle_chat_message(data):
    """WebSocket Ã¼zerinden sohbet mesajÄ± iÅŸle"""
    try:
        figure_id = data.get('figure_id')
        message = data.get('message')
        
        if figure_id in HISTORICAL_FIGURES:
            figure = HISTORICAL_FIGURES[figure_id]
            
            # Model yÃ¼klÃ¼ deÄŸilse yÃ¼kle
            if generator is None:
                load_model()
            
            # AI yanÄ±tÄ± oluÅŸtur
            ai_response = generate_response(figure, message)
            
            # YanÄ±tÄ± istemciye gÃ¶nder
            emit('ai_response', {
                'response': ai_response,
                'figure_name': figure['name'],
                'timestamp': datetime.now().isoformat(),
                'model': MODEL_CONFIG["model_name"]
            })
            
    except Exception as e:
        emit('error', {'message': str(e)})

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    print(f"ğŸš€ Tarih-i Sima Backend baÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“Š Model: {MODEL_CONFIG['model_name']}")
    print(f"ğŸŒ Port: {port}")
    print(f"ğŸ–¥ï¸ Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    
    # Modeli arka planda yÃ¼kle
    threading.Thread(target=load_model, daemon=True).start()
    
    socketio.run(app, host='0.0.0.0', port=port, debug=True)
